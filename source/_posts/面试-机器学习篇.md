---
title: 机器学习面试
date: 2020-03-29
tags: [machine learning]
categories: job
---



# 各种经典机器学习模型

## SVM

[博客园](https://www.cnblogs.com/siucaan/p/9623116.html)

SVM学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离**超平面**。**支持向量**指的是距离此超平面最近的数据点。

**最大化余量（支持向量与超平面的间的距离）也就是最小化w的模**

此处，固定了超平面和左右两个接触支持向量的超平面

- 数据线性可分时：硬间隔SVM
- 数据近似线性可分时：软间隔
- 数据低维不可分，高维可分时：核函数
- 数据进行多分类时：

### 对偶转换

对于原始问题（primal problem）而言，可以利用凸函数的函数包来进行求解，但是发现如果用对偶问题（dual ）

- 求解会变得更简单，
- 而且可以引入核函数。

[minmax_and_maxmin](https://blog.csdn.net/zhangzhengyi03539/article/details/49366447)

min_x max_y >= max_y min_x

[jianshu](https://www.jianshu.com/p/52aeaa540d25?utm_campaign)



无论 primal problem 是什么形式，dual problem 总是一个 **convex optimization 的问题**——它的极值是唯一的（如果存在的话）. 对于那些难以求解的 primal problem （甚至可以是 NP 问题），我们可以通过找出它的 dual problem ，通过优化这个 dual problem 来得到原始问题的一个下界估计。

#### SVM中的strong duality

- SVM是一种convex optimization, SVM中通过QP(quadratic programming凸二次规划)求解, QP是凸优化问题的特殊情况
- slater条件在SVM中等价于存在超平面能将数据分隔开来



### Soft

[soft](http://www.360doc.com/content/18/0526/06/36490684_757087749.shtml)

- 在目标函数中，加入了每个项的错误程度
- 在约束条件中，不再要求label*预测值>=1，而是要求>=1-错误程度。
- 这个错误程度是由在支持向量超平面的错误侧的远近来衡量的。

### 多分类

[csdn](https://blog.csdn.net/cayman_2015/article/details/90210865)

- 同时考虑所有分类：更改求解的优化问题。[zhihu](https://zhuanlan.zhihu.com/p/66933242)

- 组合多个二分类器

  - 一对一：需要对n类训练数据两两组合，构建n(n- 1)/2个支持向量机，每个支持向量机训练两种不同类别的数据，最后分类的时候采取“投票”的方式决定分类结果。

  - 一对多：n个类需要n个分类器。第k个支持向量机在第k类和其余n-1个类之间构造一个超平面，最后结果由输出离分界面距离wx+ b最大的那个支持向量机决定。

    - 优点：训练k个分类器，个数较少，其分类速度相对较快。
    - 缺点：每个分类器的训练都是将全部的样本作为训练样本，这样在求解二次规划问题时，训练速度会随着训练样本的数量的增加而急剧减慢；同时由于负类样本的数据要远远大于正类样本的数据，从而出现了**样本不对称**的情况，且这种情况随着训练数据的增加而趋向严重。解决不对称的问题可以
      - **引入不同的惩罚因子**，对样本点来说较少的正类采用较大的惩罚因子C。还有就是当有新的类别加进来时，需要对所有的模型进行重新训练。
      - 在实际情况下，我们应该合理的选择**冒认者**模型，选择出距离分类面较近的 少些样本来进行训练。

  - 层次支持向量机

    层次分类法首先将所有类别分成两个子类，再将子类进一步划分成两个次级子类，如此循环，直到得到一个单独的类别为止。

### Kernel

[zhihu](https://zhuanlan.zhihu.com/p/61794781)

[csdn](https://blog.csdn.net/ningyanggege/article/details/84072842)

[csdn-functions](https://blog.csdn.net/qq_29462849/article/details/89516133?depth_1-utm_source=distribute.pc_relevant.none-task&utm_source=distribute.pc_relevant.none-task)

[cnblogs](https://www.cnblogs.com/Key-Ky/p/5089961.html)

核方法是一类把低维空间的非线性可分问题，转化为高维空间的线性可分问题的方法。由于从输入空间到特征空间的这种映射会使得维度发生爆炸式的增长，因此特征空间的内积的运算会非常的大以至于无法承受，因此通常我们会构造一个核函数。**一个映射对应于一个核函数**。

Q: 模型型本身不是线性的，为什么一定要先用线性模型做？
 A: 有核函数和泰勒展开等等，可以无限逼近转化成线性？

核函数输入两个向量，它返回的值跟两个向量分别作 ![[公式]](https://www.zhihu.com/equation?tex=%5Cphi) 映射然后点积的结果相同。核技巧是一种利用核函数直接计算 ![[公式]](https://www.zhihu.com/equation?tex=%5Clangle+%5Cphi%28x%29%2C%5Cphi%28z%29+%5Crangle) ，以避开分别计算 ![map1](https://www.zhihu.com/equation?tex=%5Cphi%28x%29) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cphi%28z%29) ，从而**加速核方法计算**的技巧。

#### 步骤

要想构造核函数k，我们

1. 首先要确定输入空间到特征空间的映射，

2. 但是如果想要知道输入空间到映射空间的映射，我们需要明确输入空间内数据的分布情况，

3. 但大多数情况下，我们并不知道自己所处理的数据的具体分布，故一般很难构造出完全符合输入空间的核函数，因此我们常用如下几种常用的核函数来代替自己构造核函数

   - 线性核函数：![map1](https://www.zhihu.com/equation?tex=K%28x%2Cx%27%29%3Dx%5ETx%27)主要用于线性可分的情况，我们可以看到特征空间到输入空间的维度是一样的，其参数少速度快，对于线性可分数据，其分类效果很理想，因此我们通常首先尝试用线性核函数来做分类，看看效果如何，如果不行再换别的。

   - 多项式核：![map1](https://www.zhihu.com/equation?tex=K%28x%2Cx%27%29%3D%28a%2Brx%5ETx%27%29%5EQ)。含有三个参数（a,r,Q）。要注意(a,r)有范围的限制才成为一个一般的核函数。例子：对应的映射是相应的Q次的。例如：![map1](https://www.zhihu.com/equation?tex=%5Cphi%28x_1%2Cx_2%2Cx_3%29%3D%281%2Cx_1%2Cx_2%2Cx_3%5E2%2Cx_2%5E3%29)

     优点：

     - 可解决非线性问题
     - 可通过主观设置Q来实现总结的预判

     缺点：

     - 对于大量级的Q不太适用，因为会导致运算结果很大
     - 比较多的参数要选择(a,r,Q)，取决于要构造的映射（特征空间）。

   - 高斯核(RBF)：![map1](https://www.zhihu.com/equation?tex=K%28x%2Cx%27%29%3D%5Cexp%28-%5Cfrac%7B%5C%7Cx-x%27%5C%7C%5E2%7D%7B2%5Csigma%5E2%7D%29)，[zhihu](https://www.zhihu.com/question/35602879)对应的映射是：![map1](https://www.zhihu.com/equation?tex=%5Cphi%28x%29%3D%5Cexp%28-%5Cfrac%7Bx%5E2%7D%7B2%5Csigma%5E2%7D%29%5Ccdot%281%2C%5Csqrt%7B%5Cfrac%7B1%7D%7B1%21%5Csigma%5E%7B2%7D%7D%7Dx%2C%5Csqrt%7B%5Cfrac%7B1%7D%7B2%21%5Csigma%5E%7B4%7D%7D%7Dx%5E2%2C%5Ccdots%2C%5Csqrt%7B%5Cfrac%7B1%7D%7Bn%21%5Csigma%5E%7B2n%7D%7D%7Dx%5En%2C%5Ccdots%29)

     优点：

     - 可以映射到无限维
     - 决策边界更为多样
     - 只有一个参数，相比多项式核容易选择
     - 该核函数是应用最广的一个，无论大样本还是小样本都有比较好的性能，而且其相对于多项式核函数参数要少，因此大多数情况下在不知道用什么核函数的时候，优先使用高斯核函数。

     缺点：

     - 可解释性差(无限多维的转换，无法算w)
     - 计算速度比较慢(解一个对偶问题)
     - 容易过拟合(参数选不好时容易overfitting)

   - sigmoid核函数：![map1](https://www.zhihu.com/equation?tex=K%28x%2Cx%27%29%3D%5Ctanh%28%5Ceta%3Cx%2Cx%27%3E%2B%5Ctheta%29)

因此，在选用核函数的时候，如果我们对我们的数据有一定的先验知识，就利用先验来选择符合数据分布的核函数；如果不知道的话，通常使用交叉验证的方法，来试用不同的核函数，误差最下的即为效果最好的核函数，或者也可以将多个核函数结合起来，形成混合核函数。



下面是吴恩达的见解：[zhihu](https://www.zhihu.com/question/21883548/answer/112128499)

1. 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM
2. 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel
3. 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况



用过SVM就知道这个算法有多占内存，多慢，折腾半天效果还没有xgb好。[source](https://zhuanlan.zhihu.com/p/87274840)

### 面试问题

[cnblogs](https://www.cnblogs.com/siucaan/p/9623116.html)

1. SVM如何处理数据偏斜问题？

   使用不同的惩罚系数C。对数量多的类使用较小的惩罚系数，数量少的类使用较大的惩罚系数。

2. SVM的鲁棒性问题。

   有一定的噪声容忍度。噪声太大识别率会降低。

3. 结构化风险和经验风险。

   ![loss](https://www.zhihu.com/equation?tex=%5Cmin_f%5COmega%28f%29%2BC%5Csum_%7Bi%3D1%7D%5EN%20l%28f%28x_i%29%2Cy_i%29)

   - 第一项：结构风险。描述模型的性质

   - 第二项：经验风险。表述训练集上的误差。

     **正则化问题**： 正则系数C用来平衡结构风险和经验风险。常用作正则项的是范数，

     - L2范数倾向于w的分量取值尽量均衡，即非零分量的值大小尽可能差不多。
     - L0，L1范数则倾向于w分量尽量稀疏，即非零分量个数尽量少。

   

## GBDT

GBDT是一种基于boosting增强策略的加法模型，训练的时候采用前向分布算法进行贪婪的学习，每次迭代都学习一棵CART树来拟合之前 t-1 棵树的预测结果与训练样本真实值的残差。



基于梯度提升算法的学习器叫做GBM(Gradient Boosting  Machine)。理论上，GBM可以选择各种不同的学习算法作为基学习器。现实中，用得最多的基学习器是决策树。

- 为什么梯度提升方法倾向于选择决策树（通常是CART树）作为基学习器呢？

  这与决策树算法自身的优点有很大的关系。

  - 决策树可以认为是if-then规则的集合，易于理解，**可解释性强**，预测速度快。

  - 同时，决策树算法相比于其他的算法需要**更少的特征工程**，比如可以不用做特征标准化，可以很好的处理字段缺失的数据，也可以不用关心特征间是否相互依赖等。

  - 决策树能够**自动组合多个特征**，它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）。

  不过，单独使用决策树算法时，有容易过拟合缺点。所幸的是，通过各种方法，抑制决策树的复杂性，降低单颗决策树的拟合能力，再通过梯度提升的方法集成多个决策树，最终能够很好的解决过拟合的问题。由此可见，梯度提升方法和决策树学习算法可以互相取长补短，是一对完美的搭档。至于抑制单颗决策树的复杂度的方法有很多，比如限制树的最大深度、限制叶子节点的最少样本数量、限制节点分裂时的最少样本数量、吸收bagging的思想对训练样本采样（subsample），在学习单颗决策树时只使用一部分训练样本、借鉴随机森林的思路在学习单颗决策树时只采样一部分特征、在目标函数中添加正则项惩罚复杂的树结构等。



- 什么是boosting？什么是bagging？[博客园](https://www.cnblogs.com/onemorepoint/p/9264782.html)
  - boosting+Tree =>GBDT
  - bagging + Tree => Random Forest

- RF和GBDT的区别：

  - 相同点：都是由多棵树组成，最终的结果都是由多棵树一起决定。
  - 不同点：
    - **集成学习**：RF属于bagging思想，而GBDT是boosting思想
    - **偏差-方差权衡**：RF不断的降低模型的方差，而GBDT不断的降低模型的偏差
    - **训练样本**：RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本
    - **并行性**：RF的树可以并行生成，而GBDT只能顺序生成(需要等上一棵树完全生成)
    - **最终结果**：RF最终是多棵树进行多数表决（回归问题是取平均），而GBDT是加权融合
    - **数据敏感性**：RF对异常值不敏感，而GBDT对异常值比较敏感
    - **泛化能力**：RF不易过拟合，而GBDT容易过拟合

- GBDT和RF哪个比较深？[zhihu](https://zhuanlan.zhihu.com/p/82521899)

  - 第一种解释：  RF深。
    - 随机森林的思路是用大量低偏差高方差的基学习器进行集成，简单平均（不过lightgbm中的rf貌似不太一样，没有细致研究过），降低方差，所以希望每一个基学习器的精度尽量高，如果随机森林的基学习器偏差大，对于100个或者10000个精度为0.6的学习器，很难通过随机森林的集成方式来达到好的效果；
    - 而gbdt本身就是对误差的不断拟合，本身就是一个偏差很低的集成框架，那么为了同时也使得方差缩小，需要基学习器的泛化性能好一些，避免整个框架的偏差很低但方差很大的窘境；
  - 第二种解释：
    - 随机森林每一颗树都是独立的，每一颗树都是以原始标签进行训练的，在不进行任何限制的情况下会生长的比较深，
    - 而gbdt不一样，每一轮都是以上一轮的负梯度为新标签进行训练，训练到一定程度的时候我们去观察负梯度就可以发现，因为很多样本已经得到很好的拟合，所以负梯度会比较小，比如可能是这样的[0.000000001,0.000000001,0.000000001,0.0000000015......]，这样树在分裂的时候实际上再进行分裂的增益并不大，甚至分裂之后的增益反而减少，这就导致了基树训练的时候很早就停止了，从而导致树的深度降低。

- 比较GBDT和LR，说说什么情境下，GBDT不如LR:

  - 先说说LR和GBDT的区别：

    - LR是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程
    - GBDT是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合。

    当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。原因如下：**带正则化的线性模型比较不容易对稀疏特征过拟合**

    - LR 等线性模型的正则项是对权重的惩罚
    - 树模型的惩罚项通常为叶子节点数和深度等，而我们都知道，对于某些稀疏特征恰好和label相吻合的情况下，树只需要一个节点就可以完美分割正负样本，一个结点，最终产生的惩罚项极其之小。

## XGBoost

[csdn-20questions](https://blog.csdn.net/weixin_38753230/article/details/100571499?depth_1-utm_source=distribute.pc_relevant.none-task&utm_source=distribute.pc_relevant.none-task)

XGBoost对GBDT进行了一系列优化，比如损失函数进行了

- 二阶泰勒展开、
- 目标函数加入正则项、
- 支持并行：为何能并行？
- 默认缺失值处理等，

在可扩展性和训练速度上有了巨大的提升，但其核心思想没有大的变化。

![loss](https://www.zhihu.com/equation?tex=Obj%5E%7B%28t%29%7D%3D%5CSigma_%7Bi%3D1%7D%5En%20l%28y_i%2C%20%5Chat%7By%7D_i%5E%7B%28t-1%29%7D%2Bf_t%28x_i%29%29%2B%5COmega%28f_t%29%2Bconstant)

### 原理

在原来的损失函数基础上，加入了正则项。

![regular](https://www.zhihu.com/equation?tex=%5COmega%28f_t%29%3D%5Cgamma%20T%2B%5Cfrac%7B1%7D%7B2%7D%5Clambda%5CSigma_%7Bi%3D1%7D%5ET%20w_i%5E2)

将模型预测值与实际值残差作为下一棵树的输入数据。

VS GBDT:

- 导数信息：XGBoost对损失函数做了二阶泰勒展开，GBDT只用了一阶导数信息，并且XGBoost还支持自定义损失函数，只要损失函数一阶、二阶可导。

- 正则项：XGBoost的目标函数加了正则项， 相当于预剪枝，使得学习出来的模型更加不容易过拟合。正则项控制着模型的复杂度，包括了叶子节点数目T和leaf score的L2模的平方.

- 列抽样：XGBoost支持列采样（只选取一部分特征），与随机森林类似，用于防止过拟合。

- 缺失值处理：对树中的每个非叶子结点，XGBoost可以自动学习出它的默认分裂方向。如果某个样本该特征值缺失，会将其划入默认分支。[jianshu](https://www.jianshu.com/p/5b8fbbb7e754) 在寻找split point的时候，不会对该特征为missing的样本进行遍历统计，只对该列特征值为non-missing的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找split point的时间开销。在逻辑实现上，为了保证完备性，会分别处理将missing该特征值的样本分配到左叶子结点和右叶子结点的两种情形，计算增益后选择增益大的方向进行分裂即可。可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率。如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子树。

- 并行化：注意不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。

### 面试问题

- XGBoost为什么使用泰勒二阶展开
  
  [zhihu](https://zhuanlan.zhihu.com/p/82521899)

  - 形式上的统一：xgboost在对mse的损失函数设计完求解器之后，这一套代码可以直接复用给别的损失函数来使用，因为我们如果不做二阶泰勒展开的话，比如新的损失函数是二元交叉熵，在工程设计上，我们还要将损失函数的求导，然后把求导之后的式子写出来，设计一个求解器去求解。
  
  - 精准性：相对于GBDT的一阶泰勒展开，二阶泰勒展开，可以更为精准的逼近真实的损失函数
  - 可扩展性：损失函数支持自定义，只需要新的损失函数二阶可导。
  
- XGBoost为什么可以并行训练？

  - 在训练之前，每个特征按特征值对样本进行预排序，并存储为Block结构，将排序后的结构保存在内存中，这样后续分裂的时候就不需要重复对特征进行排序然后计算最佳分裂点了，并且能够进行并行化计算。
  
    在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个block并行计算。
  
- XGBoost中的一棵树的停止生长条件

  - 当新引入的一次分裂所带来的增益Gain<0时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程。
  - 当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合，这里需要设置一个超参数max_depth。
  - 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂。这涉及到一个超参数:最小样本权重和，是指如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细。
  
- XGBoost如何处理不平衡数据

  - 第一种，如果你在意AUC，采用AUC来评估模型的性能，那你可以通过设置scale_pos_weight来平衡正样本和负样本的权重。例如，当正负样本比例为1:10时，scale_pos_weight可以取10；
  - 第二种，如果你在意概率(预测得分的合理性)，你不能重新平衡数据集(会破坏数据的真实分布)，应该设置max_delta_step为一个有限数字来帮助收敛（基模型为LR时有效）。

- XGBoost如何进行剪枝？

  - 在目标函数中增加了正则项：使用叶子结点的数目和叶子结点权重的L2模的平方，控制树的复杂度。
  - **信息增益**： 在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂。
  - **权重和**： 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会放弃此次分裂。
  - **树的深度**： XGBoost 先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝。

- XGBoost如何评价特征重要性？

  - weight ：the number of times a feature is used to split the data across all trees. 该特征在所有树中被用作分割样本的特征的总次数。
  
  - gain - the average gain of the feature when it is used in trees. 	该特征在其出现过的所有树中产生的平均增益。[csdn](https://blog.csdn.net/GreatMichael001/article/details/84392314)
  
  - cover - the average coverage of the feature when it is used in trees.该特征在其出现过的所有树中的平均覆盖范围。
  
    覆盖范围这里指的是一个特征用作分割点后，其影响的样本数量，即有多少样本经过该特征分割到两个子节点。
  
- XGBoost模型如果过拟合了怎么解决？

  - **调参**！ 当出现过拟合时，有两类参数可以缓解：

    - 第一类参数：用于直接控制模型的复杂度。包括`max_depth,min_child_weight,gamma` 等参数

    - 第二类参数：用于增加随机性，从而使得模型在训练时对于噪音不敏感。包括`subsample,colsample_bytree`

    还有就是直接减小`learning rate`，但需要同时增加`estimator` 参数。

- 为什么XGBoost相比某些模型对缺失值不敏感？

  - 一些模型如SVM和KNN，其模型原理中涉及到了对样本距离的度量，如果缺失值处理不当，最终会导致模型预测效果很差。
  - 而树模型，一棵树中每个结点在分裂时，寻找的是某个特征的最佳分裂点（特征值），完全可以不考虑存在特征值缺失的样本，也就是说，如果某些样本的特征值缺失，对寻找最佳分割点的影响不是很大。
  
- rf和xgb哪个对异常点更敏感？

  [zhihu](https://zhuanlan.zhihu.com/p/82521899)

  xgb明显敏感的多，当然对rf也是有一定影响的，rf的每棵数的生成是独立的，异常点数量不多的情况下异常点常常和正常样本中的某些样本合并在一个分支里。

  但是xgb不一样，异常样本的t-1轮的预测值和真实标签计算出来的负梯度会一直很大，假设当到达某一轮的时候，所有正常样本的计算得到的负梯度都很小而异常样本的负梯度很大例如【0.0000001,0.0000001,0.0000001,0.0000001,0.0000001,10】,这个时候新树会可能会继续进行不正常的分裂为[0.0000001,0.0000001,0.0000001,0.0000001,0.0000001],[10]，而这样的分裂是不合理的，因为异常值本身可能是因为某些人为失误导致的数据记录错误，或者异常样本完全是属于另外一种分布，此时强制要进行模型训练会导致模型的结果有偏从而发生过拟合。

  当然异常样本数量很少比如10个以内的时候而正常样本有100000000个其实基本没什么影响，但是如果占比较高的话是会产生影响的。

- XGB中l1和l2是怎么用的？

  - l1表示对叶节点个数的约束项的系数，
  - l2则是叶子节点权重的约束项系数。

（超纲项）
- XGBoost 为什么这么快
  -  分块并行：训练前每个特征按特征值进行排序并存储为Block结构，后面查找特征分割点时重复使用，并且支持并行查找每个特征的分割点
  - 候选分位点：每个特征采用常数个分位点作为候选分割点
  - CPU cache 命中优化： 使用缓存预取的方法，对每个线程分配一个连续的buffer，读取每个block中样本的梯度信息并存入连续的Buffer中。
  - Block 处理优化：Block预先放入内存；Block按列进行解压缩；将Block划分到不同硬盘来提高吞吐。
- XGBoost如何选择最佳分割点？
  - 在训练前预先将特征按照特征值进行了排序，并存储为block结构，以后在结点分裂时可以重复使用该结构。
  - 采用特征并行的方法利用多个线程分别计算每个特征的最佳分割点，根据每次分裂后产生的增益，最终选择增益最大的那个特征的特征值作为最佳分裂点。
  - 如果在计算每个特征的最佳分割点时，对每个样本都进行遍历，计算复杂度会很大，这种全局扫描的方法并不适用大数据的场景。XGBoost还提供了一种直方图近似算法，对特征排序后仅选择常数个候选分裂位置作为候选分裂点，极大提升了结点分裂时的计算效率
- XGBoost的可扩展性是如何实现的：
  - **基分类器的scalability**：弱分类器可以支持CART决策树，也可以支持LR和Linear。
  - **目标函数的scalability**：支持自定义loss function，只需要其一阶、二阶可导。有这个特性是因为泰勒二阶展开，得到通用的目标函数形式。
  - **学习方法的scalability**：Block结构支持并行化，支持 Out-of-core计算。

### 调参

[zhihu](https://zhuanlan.zhihu.com/p/35061092)

调参的基本步骤：

1. 初始化一些基本量：max_depth=5,min_child_weight=1, gamma = 0,subsample,colsample_bytree = 0.8, scale_pos_weight = 1

2. 一些惯用顺序：

   1. 确定learning_rate和n_estimator：learning rate可以先用0.1，用cv来寻找最优的estimators

   2. max_depth和min_child_weight。我们调整这两个参数是因为，这两个参数对输出结果的影响很大。我们首先将这两个参数设置为较大的数，然后通过迭代的方式不断修正，缩小范围。

      - max_depth，每棵子树的最大深度，check from range(3,10,2)。

      - min_child_weight，子节点的权重阈值，check from range(1,6,2)。如果一个结点分裂后，它的所有子节点的权重之和都大于该阈值，该叶子节点才可以划分。

   3. gamma。也称作最小划分损失`min_split_loss`，check from 0.1 to 0.5，指的是，对于一个叶子节点，当对它采取划分之后，损失函数的降低值的阈值。

      - 如果大于该阈值，则该叶子节点值得继续划分
      - 如果小于该阈值，则该叶子节点不值得继续划分

   4. subsample, colsample_bytree

      - subsample是对训练的采样比例

      - colsample_bytree是对特征的采样比例

      both check from 0.6 to 0.9

   5. 正则化参数：reg_alpha以及reg_lambda: 

      alpha 是L1正则化系数，try 1e-5, 1e-2, 0.1, 1, 100

      lambda 是L2正则化系数。这两者出现在衡量树的正则项（复杂度）中。叶子节点的权重计算可以采取L1或L2.

   6. 最后就是learning_rate，一般这时候要调小学习率来测试。降低学习率的同时增加树的数量，通常最后设置学习率为0.01~0.1？？？？

其实调参对于模型准确率的提高有一定的帮助，但这是有限的。

**最重要的还是要通过数据清洗，特征选择，特征融合，模型融合等手段来进行改进！**

### 优点

- 正则化

- 并行处理

- 高度灵活性

- 缺失值处理

- 剪枝：

  - 当分裂时遇到一个负损失时，GBM会停止分裂。因此GBM实际上是一个**贪心算法**。

  - XGBoost会一直分裂到指定的最大深度(max_depth)，然后回过头来剪枝。如果某个节点之后不再有正值，它会去除这个分裂。

    这种做法的优点，当一个负损失（如-2）后面有个正损失（如+10）的时候，就显现出来了。GBM会在-2处停下来，因为它遇到了一个负值。但是XGBoost会继续分裂，然后发现这两个分裂综合起来会得到+8，因此会保留这两个分裂。

## LightGBM

[csdn](https://blog.csdn.net/weixin_41510260/article/details/95378749)

LightGBM是一个实现GBDT**算法的框架**，支持高效率的并行训练。

### 算法优化

- 基于Histogram的决策树算法
- 直方图做差加速直接
- 基于直方图的稀疏特征优化多线程优化。
- 支持类别特征(Categorical Feature)
- Cache命中率优化
- 带深度限制的Leaf-wise的叶子生长策略



#### 直方图算法

直方图算法的基本思想是

1. 先把连续的浮点特征值离散化成k个整数（其实就是分桶的思想）
2. 在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。

优点：

- 最明显就是内存消耗的降低，直方图算法不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值，而这个值一般用8位整型存储就足够了，内存消耗可以降低为原来的1/8。
- 计算代价大幅降低：预排序算法对每个特征，需要遍历每一个数据点来计算一次分裂的增益，而直方图算法只需要计算k次（k可以认为是常数），时间复杂度从O(#data\*#feature)优化到O(k*#features)。

#### 直方图做差

一个容易观察到的现象：一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。

LightGBM在构造一个叶子的直方图后，用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。

#### Leaf-wise生长策略

有两种常用的生长策略：Level-wise和Leaf-wise。区别类似于（深搜和广搜）

- Level-wise：例子：XGBoost。树是按层生长的，同一层的所有节点都做分裂，最后剪枝。

  Level-wise过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种**低效**的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。

- Leaf-wise：例子：LGB。更为高效的策略：每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。

  因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。

  Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。

#### 直接支持类别特征

实际上大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，转化到多维的0/1特征，降低了空间和时间的效率。而类别特征的使用是在实践中很常用的。基于这个考虑，LightGBM优化了对类别特征的支持，可以直接输入类别特征，不需要额外的0/1展开。并在决策树算法上增加了类别特征的决策规则。在Expo数据集上的实验，相比0/1展开的方法，训练速度可以加速8倍，并且精度一致。据我们所知，LightGBM是第一个直接支持类别特征的GBDT工具。

#### 支持并行化

[csdn](https://blog.csdn.net/huacha__/article/details/81057150#%E6%94%AF%E6%8C%81%E5%B9%B6%E8%A1%8C%E5%AD%A6%E4%B9%A0)

LightGBM原生支持并行学习，目前支持**特征并行(Featrue Parallelization)**和**数据并行(Data Parallelization)**两种，还有一种是**基于投票的数据并行(Voting Parallelization)**

- **特征并行**的主要思想是在不同机器、在**不同的特征集合**上分别寻找最优的分割点，然后在机器间同步最优的分割点。
- **数据并行**则是让不同的机器先在本地构造直方图，然后进行全局的合并，最后**在合并的直方图上面**寻找最优分割点。

LightGBM针对这两种并行方法都做了优化。

- **特征并行**算法中，通过在本地保存全部数据避免对数据切分结果的通信。
- **数据并行**中使用分散规约 (Reduce scatter) 把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。
- **基于投票的数据并行(Voting Parallelization)**则进一步优化数据并行中的通信代价，使通信代价变成常数级别。在数据量很大的时候，使用投票并行可以得到非常好的加速效果。

### 评价

具有以下优点：

- 更快的训练速度
- 更低的内存消耗
- 更好的准确率
- 分布式支持，可以快速处理海量数据

## NB

[cnblogs](https://www.cnblogs.com/itmorn/p/7905975.html)

朴素贝叶斯(naive Bayes) 法是基于

- 贝叶斯定理与

- 特征条件独立假设(Naive)的分类方法。

  对于给定的训练数据集，

  1. 首先基于特征条件独立假设学习输入/输出的联合概率分布；
  2. 然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。

工作流程：

- 训练：先根据数据集，
  - 计算标记的先验概率，
  - 再计算每一个特征的条件概率。
- 预测：给定一个新的数据点：
  - 计算各个类别下的后验概率
  - 贝叶斯模型会将实例分到后验概率最大的类中。

如何处理连续型变量？

- 使用高斯贝叶斯分布。假设连续型变量服从高斯分布，拟合参数，再计算”后验概率“。

# NLP

## Representation

[csdn](https://blog.csdn.net/u014038273/article/details/80727442)

- one-hot 编码：传统的独热表示（ one-hot representation）仅仅将词符号化，不包含任何语义信息。
- distributed 编码：结合了上下文的信息：词的语义由其上下文决定。

根据建模的不同，主要可以分为三类：

- 基于矩阵的分布表示。代表： GloVe模型。在这种表示下，两个词的语义相似度可以直接转化为两个向量的空间距离。

- 基于聚类的分布表示

- 基于神经网络的分布表示。

尽管这些不同的分布表示方法使用了不同的技术手段获取词表示，但由于这些方法均基于分布假说，它们的核心思想也都由两部分组成：

- 选择一种方式描述上下文；
- 选择一种模型刻画某个词（下文称“目标词”）与其上下文之间的关系。

### Word2Vec

[jianshu](https://www.jianshu.com/p/471d9bfbd72f)

实现CBOW（ Continuous Bag-of-Words）和 Skip-gram 语言模型的工具正是well-known word2vec。另外，C&W 模型的实现工具是SENNA。

- CBOW模型的训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。
- Skip-Gram模型和CBOW的思路是反着来的，即输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。
- CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好。

例子：

以“我爱北京天安门”这句话为例。假设我们现在关注的词是“爱”，C＝2时它的上下文分别是“我”，“北京天安门”。

- 输入：CBOW模型就是把“我”  “北京天安门” 的one  hot表示方式作为输入，也就是C个1xV的向量，

- 操作：分别跟同一个VxN的大小的系数矩阵W1相乘得到C个1xN的隐藏层hidden  layer，然后C个取平均所以只算一个隐藏层。这个过程也被称为线性激活函数(这也算激活函数？分明就是没有激活函数了)。然后再跟另一个NxV大小的系数矩阵W2相乘得到1xV的输出层，

- 损失计算：这个输出层每个元素代表的就是词库里每个词的事后概率。输出层需要跟ground truth也就是“爱”的one  hot形式做比较计算loss。

- 计算优化：这里需要注意的就是V通常是一个很大的数比如几百万，计算起来相当费时间，除了“爱”那个位置的元素肯定要算在loss里面，word2vec就用基于huffman编码的Hierarchical softmax筛选掉了一部分不可能的词，然后又用**negative  sampling**再去掉了一些负样本的词所以时间复杂度就从O(V)变成了O(logV)。

  Skip gram训练过程类似，只不过输入输出刚好相反。

无监督或弱监督的预训练以word2vec和auto-encoder为代表。这一类模型的特点是，不需要大量的人工标记样本就可以得到质量还不错的embedding向量。

### 负采样

在训练神经网络时，每当接受一个训练样本，然后调整所有神经单元权重参数，来使神经网络预测更加准确。换句话说，每个训练样本都将会调整所有神经网络中的参数。

negative sampling  每次让一个训练样本仅仅更新一小部分的权重参数，从而降低梯度下降过程中的计算量。

negative sampling 的想法也很直接 ，将随机选择一小部分的 negative words，比如选 10个 negative words 来更新对应的权重参数。

在论文中作者指出对于小规模数据集，建议选择 5-20 个negative words，对于大规模数据集选择 2-5个 negative words.

使用 一元模型分布 (unigram distribution) 来选择 negative words，一个单词被选作 negative sample 的概率跟它出现的频次有关，出现频次越高的单词越容易被选作negative words。经验公式为：

![negative](https://www.zhihu.com/equation?tex=%5CPr%28w_i%29%3D%5Cfrac%7Bf%28w_i%29%5E%7B3%2F4%7D%7D%7B%5Csum_i%20f%28w_i%29%5E%7B3%2F4%7D%7D)



## 面试问题

- 什么是one-hot编码？

  - one-hot编码就是保证每个样本中的单个特征只有1位处于状态1，其他的都是0。

- 什么是word embedding（词嵌入）？

  - 将高维词向量嵌入到一个低维空间。

- 对比 Skip-gram 和 CBOW

  [csdn](https://blog.csdn.net/wisimer/article/details/104688095/)

  - 训练速度上 CBOW 应该会更快一点。因为每次会更新 context(w) 的词向量，而 Skip-gram 只更新核心词的词向量。两者的预测时间复杂度分别是 O(V)，O(KV)
  - 对低频词的效果：Skip-gram 对低频词效果比 CBOW好。因为是尝试用当前词去预测上下文，当前词是低频词还是高频词没有区别。但是 CBOW  相当于是完形填空，会选择最常见或者说概率最大的词来补全，因此不太会选择低频词。Skip-gram  在大一点的数据集可以提取更多的信息。

# 深度学习

[cnblogs](https://www.cnblogs.com/siucaan/p/9623115.html)

## 结构元件

## 算法

### 反向传播

### Batch Normalization

[cnblogs](https://www.cnblogs.com/guoyaohua/p/8724433.html)

BN的基本思想其实相当直观：因为深层神经网络在做非线性变换前的**激活输入值**（就是那个x=WU+B，U是输入）**随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近**（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这**导致反向传播时低层神经网络的梯度消失**，这是训练深层神经网络收敛越来越慢的**本质原因**，**而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布**，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是**这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。**

其实一句话就是：**对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。**

**经过BN后，目前大部分Activation的值落入非线性函数的线性区内，其对应的导数远离导数饱和区，这样来加速训练收敛过程。**

核心思想应该是想找到一个线性和非线性的较好平衡点，既能享受非线性的较强表达能力的好处，又避免太靠非线性区两头使得网络收敛速度太慢。



好处：

- 不仅仅极大提升了训练速度，收敛过程大大加快；
- 还能增加分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果；
- 另外调参过程也简单多了，对于初始化要求没那么高，而且可以使用大的学习率等。

## 经典模型

### CNN

[csdn](https://blog.csdn.net/qq_38906523/article/details/79822958)

- CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN解出来？为什么AlphaGo里也用了CNN？这几个不相关的问题的相似性在哪里？CNN通过什么手段抓住了这个共性？

  几个不相关的问题的相关性在于，都存在局部与整体的关系，由低层次的特征经过组合，组成高层次的特征，并且得到不同特征之间的空间相关性。

  CNN通过：局部感知、权值共享、池化操作、多层次结构抓住了这个共性。

  - 局部感知使网络可以提取数据的局部特征；
  - 权值共享大大降低了网络的训练难度；
  - 池化操作和多层次结构一起，实现了数据的降维，将低层次的特征组合成高层次的特征。

### ResNet

相比较于以前网络的直来直去结构，残差中有很多**跨层连接**结构

## 面试问题

[csdn](https://blog.csdn.net/humanpose/article/details/94150396)

- 解决梯度消失爆炸的主要方案：
  - 预训练加微调
  - 梯度剪切、权重正则（针对梯度爆炸）
  - 使用不同的激活函数
  - 使用batchnorm
  - 使用残差结构
  - 使用LSTM网络


# 模型流程

## 数据预处理

[cnblogs](https://www.cnblogs.com/siucaan/p/9623114.html)

### 数据标准化/归一化

- 什么是数据标准化/归一化？[csdn](https://blog.csdn.net/weixin_36604953/article/details/102652160)

  - 归一化：将一列数据变化到某个固定区间(范围)中，通常，这个区间是[0, 1]，广义的讲，可以是各种区间，比如映射到[0，1]一样可以继续映射到其他范围，图像中可能会映射到[0,255]，其他情况可能映射到[-1,1]。公式：![normalization](https://www.zhihu.com/equation?tex=%5Cfrac%7BX-X_%7Bmin%7D%7D%7BX_%7Bmax%7D-X_%7Bmin%7D%7D)
  - 标准化：将数据变换为均值为0，标准差为1的分布切记，并非一定是正态的。公式：![standard](https://www.zhihu.com/equation?tex=%5Cfrac%7BX-%5Cmu%7D%7B%5Csigma%7D)
  - 中心化：将数据变为均值为0。公式：![standard](https://www.zhihu.com/equation?tex=X-%5Cmu)

- 为什么要做数据标准化/归一化？

  - 特征变量的量纲和数值的量级不一样，与参数/模型结构相互作用，会对目标变量产生不一样的影响。
  - 通过标准化处理，可以使得不同的特征变量具有相同的尺度（也就是说将特征的值控制在某个范围内），这样目标变量就可以由多个相同尺寸的特征变量进行控制。由此，在使用梯度下降法学习参数的时候，不同特征对参数的影响程度就一样了。比如在训练神经网络的过程中，通过将数据标准化，能够加速权重参数的收敛。数据归一化后，**最优解的寻优过程明显会变得平缓，更容易正确的收敛到最优解。**[zhihu](https://zhuanlan.zhihu.com/p/27627299)
  - 简而言之：对数据标准化的目的是消除特征之间的差异性，便于特征一心一意学习权重。
  - 这取决于模型中，特征变量和参数是如何相互作用来影响目标变量的。

- 什么时候需要做数据标准化/归一化？

  [csdn](https://blog.csdn.net/weixin_36604953/article/details/102652160)

  - 机器学习任务和统计学任务中有很多地方要用到**“距离”的计算**，比如PCA，比如KNN，比如kmeans等等，假使算欧式距离，不同维度量纲不同可能会导致距离的计算依赖于量纲较大的那些特征而得到不合理的结果；例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）。 

  - 参数估计时**使用梯度下降**，在使用梯度下降的方法求解最优化问题时， 归一化/标准化后可以加快梯度下降的求解速度，即提升模型的收敛速度。

  - 当原始数据不同维度特征的尺度(量纲)不一致时，需要标准化步骤对数据进行标准化或归一化处理，反之则不需要进行数据标准化。

  - 不是所有的模型都需要做归一的，比如

    - 模型算法里面有没关于对距离的衡量，

    - 没有关于对变量间标准差的衡量。

      比如决策树采用的算法里面没有涉及到任何和距离等有关的，所以在做决策树模型时，通常是不需要将变量做标准化的；

      另外，概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率。

  [blog](https://www.cnblogs.com/shine-lee/p/11779514.html)

  ![featurescaling](\pic\featurescaling.png)

  - 涉及或隐含**距离计算**的算法，比如K-means、KNN、PCA、SVM等，一般需要feature scaling，因为

    - **zero-mean一般可以增加样本间余弦距离或者内积结果的差异**，区分力更强，假设数据集集中分布在第一象限遥远的右上角，将其平移到原点处，可以想象样本间余弦距离的差异被放大了。在模版匹配中，zero-mean可以明显提高响应结果的区分度。
    - 就欧式距离而言，**增大某个特征的尺度，相当于增加了其在距离计算中的权重，如果有明确的先验知识表明某个特征很重要，那么适当增加其权重可能有正向效果，但如果没有这样的先验，或者目的就是想知道哪些特征更重要，那么就需要先feature scaling，对各维特征等而视之**。
    - 增大尺度的同时也增大了该特征维度上的方差，PCA算法倾向于关注方差较大的特征所在的坐标轴方向，其他特征可能会被忽视，因此，在PCA前做Standardization效果可能更好

  - 损失函数中含有**正则项**时，一般需要feature scaling：

    对于线性模型*y*=wx+b而言，x的任何线性变换（平移、放缩），都可以被w和b”吸收“掉，理论上，不会影响模型的拟合能力。但是，如果损失函数中含有正则项，如*λ*||w||2，λ为超参数，其对w的每一个参数施加同样的惩罚，但对于某一维特征xi而言，其scale越大，系数wi越小，其在正则项中的比重就会变小，相当于对wi惩罚变小，即损失函数会相对忽视那些scale增大的特征，这并不合理，所以需要feature scaling，使损失函数平等看待每一维特征。

  - **梯度下降算法，需要feature scaling**。梯度下降的参数更新公式如下，

    ![gradient](https://www.zhihu.com/equation?tex=W%28t%2B1%29%3DW%28t%29-%5Ceta%5Cfrac%7Bd%20E%28W%28t%29%29%7D%7BW%28t%29%7D)

    

    **收敛速度取决于：参数的初始位置到local minima的距离，以及学习率η的大小。**一维情况下，在local minima附近，不同学习率对梯度下降的影响如下图所示：

    ![gradient](/pic/gradient.png)

    收敛意味着在每个参数维度上都取得极小值，每个参数维度上的偏导数都为0，是每个参数维度上的下降速度是不同的，为了每个维度上都能收敛，学习率应取所有维度在当前位置合适步长中最小的那个。feature scaling改变了损失函数的形状，减小不同方向上的曲率差异。scaling后不同方向上的曲率相对更接近，更容易选择到合适的学习率，使下降过程相对更稳定。

- 选择标准化还是归一化？

  [csdn](https://blog.csdn.net/young951023/article/details/78389445 )

  - 在分类、聚类算法中，需要**使用距离**来度量相似性的时候、或者使用PCA技术进行降维的时候，标准化(Z-score standardization)表现更好。
  - 在不涉及距离度量、协方差计算、数据不符合正太分布的时候，可以使用第一种方法或其他归一化方法。比如图像处理中，将RGB图像转换为灰度图像后将其值限定在[0 255]的范围。
  - 总结来说，在算法、后续计算中涉及距离度量(聚类分析)或者协方差分析(PCA、LDA等)的，同时数据分布可以近似为状态分布，应当使用0均值的归一化方法。其他应用中更具需要选用合适的归一化方法。

## 特征提取

[cnblogs](https://www.cnblogs.com/siucaan/p/9623120.html)

## 优化

[cnblogs](https://www.cnblogs.com/siucaan/p/9623119.html)

## 模型融合

[cnblogs](https://www.cnblogs.com/alan-blog-TsingHua/p/10903018.html)

集成方法的思想是通过将这些弱学习器的偏置和/或方差结合起来，从而创建一个「强学习器」（或「集成模型」），从而获得更好的性能。

很重要的一点是：我们对弱学习器的选择应该和我们聚合这些模型的方式相一致。

- 如果我们选择具有低Bias高Variance的基础模型，我们应该使用一种倾向于减小Variance的聚合方法；
- 而如果我们选择具有低Variance高Bias的基础模型，我们应该使用一种倾向于减小Bias的聚合方法。

同质/异质：使用相同（不同）机器学习算法。

### Bagging

通常考虑的是同质弱学习器，相互独立地并行学习这些弱学习器，并按照某种确定性的平均过程将它们组合起来。

可以使用并行化的方法

目标是生成比单个模型更鲁棒的集成模型，减小方差Variance。

### Boosting

通常考虑的也是同质弱学习器。它以一种高度自适应的方法顺序地学习这些弱学习器（每个基础模型都依赖于前面的模型），并按照某种确定性的策略将它们组合起来。

思想是「迭代地」拟合模型，使模型在给定步骤上的训练依赖于之前的步骤上拟合的模型。

它生成的集成模型通常比组成该模型的弱学习器bias更小。

由于其重点在于减小bias，用于 boosting 的基础模型通常是那些低bias高variance的模型。例如，如果想要使用树作为基础模型，我们将主要选择只有少许几层的较浅决策树。

一旦选定了弱学习器，我们仍需要定义它们的

- 拟合方式（在拟合当前模型时，要考虑之前模型的哪些信息？）
- 聚合方式（如何将当前的模型聚合到之前的模型中？）

### Stacking

通常考虑的是异质弱学习器，并行地学习它们，并通过训练一个「元模型」将它们组合起来，根据不同弱模型的预测结果输出一个最终的预测结果。

我们需要定义两个东西：

- 想要拟合的 L 个学习器
- 组合它们的元模型。

例如，对于分类问题来说，我们可以选择 KNN 分类器、logistic 回归和SVM 作为弱学习器，并决定学习神经网络作为元模型。然后，神经网络将会把三个弱学习器的输出作为输入，并返回基于该输入的最终预测。

#### 步骤

1. 将训练数据分为两组
2. 选择 L 个弱学习器，用它们拟合第一组数据
3. 使 L 个学习器中的每个学习器对第二组数据中的观测数据进行预测
4. 在第二组数据上拟合元模型，使用弱学习器做出的预测作为输入

为了克服数据一分为二造成的数据集过小的问题，我们可以使用某种「k-折交叉训练」方法：

换句话说，它会在 k-1 折数据上进行训练，从而对剩下的一折数据进行预测。迭代地重复这个过程，就可以得到对任何一折观测数据的预测结果。这样一来，我们就可以为数据集中的每个观测数据生成相关的预测，然后使用所有这些预测结果训练元模型。

这样所有的观测数据都可以用来训练元模型：对于任意的观测数据，弱学习器的预测都是通过在 k-1 折数据（不包含已考虑的观测数据）上训练这些弱学习器的实例来完成的。

## 评估

[cnblogs](https://www.cnblogs.com/siucaan/p/9623118.html)

# 统计



## 面试问题

- MLE与MAP之间的区别与联系

  - [csdn](https://blog.csdn.net/CoderPai/article/details/88371499)MLE是 MAP 的一个特例。当假设先验分布是均匀分布的时候，MLE就是MAP。

  - MLE：假设有一批数据(样本)，且这些数据(样本)服从某个分布( 模型已知)，但是参数未知。MLE的思想就是找到一个参数值，使得每条样本出现的概率最大。

    ![mle](https://www.zhihu.com/equation?tex=%5Carg%5Cmax_%7B%5Ctheta%7D%5CPr%28x_1%2Cx_2%2C%5Ccdots%2Cx_n%7C%5Ctheta%29)

  - MAP：假设有一批数据(样本)，且这些数据(样本)服从某个分布( 模型已知)，但是参数未知。有一个额外的信息：我们虽然不知道参数具体是多少,但是我们知道这个参数也服从某个分布。MAP就是加上这个条件后,去对参数进行估计.

    ![mle](https://www.zhihu.com/equation?tex=%5Carg%5Cmax_%7B%5Ctheta%7D%5CPr%28%5Ctheta%7Cx_1%2Cx_2%2C%5Ccdots%2Cx_n%29%3D%5Carg%5Cmax_%7B%5Ctheta%7D%5Cfrac%7B%5CPr%28x_1%2Cx_2%2C%5Ccdots%2Cx_n%7C%5Ctheta%29%5CPr%28%5Ctheta%29%7D%7B%5CPr%28x_1%2Cx_2%2C%5Ccdots%2Cx_n%29%7D)

  - 从贝叶斯的角度，所有的正则项都是源于对先验的假设。详见：其他->L1、L2正则化

- 

# 其他

## L1、L2正则化

[csdn](https://blog.csdn.net/sinat_34971932/article/details/103489235)

- [x] 不理解 [maybe-solution](https://www.cnblogs.com/heguanyou/p/7688344.html)

L1： ||x||   -->  产生稀疏解
 L2 ： ||x||2   —> 压缩效应，权重参数k变得很小，变得很光滑
 从贝叶斯角度（先验角度）理解L1，L2 ：
 L1正则化： 相当于对数据加了一个先验  Laplace(拉普拉斯)先验
 L2正则化： 相当于对数据加了一个先验 高斯先验（Gaussian分布）

## 有缺失值的数据处理

对于有缺失值的数据在经过缺失处理后：

- 当数据量很小时，优先用朴素贝叶斯
- 数据量适中或者较大，用树模型，优先XGBoost
- 数据量较大，也可以用神经网络
- 避免使用距离度量相关的模型，如KNN和SVM

## 排序

[animate](https://www.cnblogs.com/javafirst0/p/11190060.html)

## 信息论

[csdn](https://blog.csdn.net/zhao_cq/article/details/80933918?depth_1-utm_source=distribute.pc_relevant.none-task&utm_source=distribute.pc_relevant.none-task)

- 熵、联合熵、条件熵、交叉熵、KL散度（相对熵），信息增益，互信息，信息增益率的计算 

  - 熵用于衡量不确定性，所以均分的时候熵最大 。从编码的角度理解：对A事件中的随机变量进行编码所需的最小字节数。

    ![entropy](https://www.zhihu.com/equation?tex=S%28A%29%3D-%5Csum_%7Ba%7DP%28a%29%5Clog%20P%28a%29%3D-%5Cint%20f%28x%29%5Clog%20f%28x%29dx)

  - KL散度用于度量两个分布的不相似性，等于交叉熵-熵。从编码的角度理解：使用B作为密码本来表示A，需要的额外的字节数。

    ![kl-divergence](https://www.zhihu.com/equation?tex=KL%28A%5C%7CB%29%3DH%28A%2CB%29-S%28A%29)

    是不对称的。变形JS散度是对称的：![js-divergence](https://www.zhihu.com/equation?tex=JS%28A%2CB%29%3D%5Cfrac%7BKL%28A%5C%7CB%29%2BKL%28B%5C%7CA%29%7D%7B2%7D)

  - 交叉熵：![cross-entropy](https://www.zhihu.com/equation?tex=H%28A%2CB%29%3D-%5Csum_%7Bx_i%7DP_A%28x_i%29%5Clog%20P_B%28x_i%29)

    从编码的角度理解：使用B作为密码本来编码A需要的最小字节数。

    **注意**： 交叉熵 中的A，B指的是两个分布，这里只有一个随机变量x。而互信息中X，Y指的是两个随机变量。

    - [ ] 到底是B作为密码本还是A作为密码本？？

  - 联合熵：H(X,Y)表示随机变量X,Y同时发生的不确定性，是对称的。

  - 条件熵：在已知一个变量发生的条件下，另一个变量发生所新增加的不确定性：

    - H(Y|X=x)是X取值为x时随机变量y的熵，
    - H(Y|X)为当X遍历所有取值时，Y的熵的期望。
    - H(Y|X)=H(X,Y)-H(X)可以理解为，X,Y同时发生的不确定性，减去X发生的不确定性。

  - 互信息Mutual information ：表示观察到x后，y的熵会减少多少![cross-entropy](https://www.zhihu.com/equation?tex=I%28X%3BY%29%3DH%28Y%29-H%28Y%7CX%29%3DH%28X%29-H%28X%7CY%29%3DH%28X%2CY%29-H%28X%7CY%29-H%28Y%7CX%29)

    - 对称的
    - H(X,Y)表示的是联合熵，而不是交叉熵。

  - [x] 没有理解交叉熵和KL散度的区别与联系[csdn](https://blog.csdn.net/Dby_freedom/article/details/83374650)

- 信息增益有什么缺点？

  信息增益的大小是相对训练数据而言的，没有绝对的意义。 
  在分类困难时，也就是说在训练数据集的经验熵大的时候，信息增益值会偏大，反之则偏小。 
  使用**信息增益比**可以矫正这一缺点。



- [ ] [zhihu](https://zhuanlan.zhihu.com/p/32422406)

## 面试问题

- Python中，list底层始如何实现的？

  在CPython中，列表被实现为长度可变的数组。

- [ ] l1求导问题

- 牛顿法与梯度下降法的区别：

  [cnblogs](https://www.cnblogs.com/happylion/p/4172632.html)

  - 梯度下降法：利用一阶导的信息，自变量按照预定的步长进行变化

  - 牛顿法：利用一阶导和**二阶导**的信息。

    [detail-csdn](https://blog.csdn.net/mushuiliu/article/details/93330403)

    - 牛顿法求根是利用一阶信息。
    - 牛顿法优化求的是一阶为0的根，所以是二阶的。为什么要求一阶为0的根呢？因为我们要找一个x来最小化f(x)。对于凸优化问题，f(x)的最小值点就是f(x)的极值点，也就是导数为0（一阶为0）的点

  - 二者速度比较：从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。

    更通俗地说，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了局部的最优，没有全局思想。）

    根据wiki上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。