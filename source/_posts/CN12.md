---
title: Exponential random graph model
date: 2019-08-09
tags: [complex system, network]
categories: course notes
---

# Objectives

using ERGM model to generate statistical ensembles with arbitrary fixed properties.

# Questions

- For which microstate probabilities is the entropy of a statistical ensemble maximized?
  - the one best describe the given knowledge
- How is the microstate probability in the exponential random graph model defined?
- What is the basic underlying assumption behind this microstate probability?
- How is the G(n,p) model related to ERGM?
  - using p as the expected property.
- How can we maximize the likelihood function in the ERGM?
  - derivative the log-likelihood function
- How can the parameters of an ERGM fit be interpreted?
  - the positive/negative effect of the factors

# Keywords

- microstate probabilities
- statistical ensemble (e.g. G(n,p) model)
- ERGM

# Microstate probability

## Definition

The probability of each microstate (e.g. a network of a network generation model) in the ensemble model.

For instance, in G(n,m) model, each microstate has the same probability 

![img](http://latex.codecogs.com/svg.latex?P%3D%5Cfrac%7B1%7D%7B%7B%7Bn%2B1%5Cchoose%202%7D%5Cchoose%20m%7D%7D)

## Maximum entropy principle

The **probability distribution** which maximizes the entropy is the best one to describe the given state of knowledge.

Use ![img](http://latex.codecogs.com/svg.latex?%5CPi) to denote a probability distribution. The MEP gives 

![img](http://latex.codecogs.com/svg.latex?%5CPi%5E%2A%3D%5Carg_%7B%5CPi%7D%5Cmax%20H%28%5CPi%29%3D%5Carg_%7B%5CPi%7D%5Cmax%20-%5Csum_%7BG%5Cin%5COmega%7D%20P%28G%29%5Clog%28P%28G%29%29)

- \Omega denotes the given state of knowledge.  For example \Omega restricts all the networks have m edges.
- each \Pi gives  a set of P()

[refer to this website](https://www.statisticshowto.datasciencecentral.com/maximum-entropy-principle/)This will be the system with the **largest remaining uncertainty**, and by choosing it you’re making sure you’re not adding any extra [biases ](https://www.statisticshowto.datasciencecentral.com/what-is-bias/)or uncalled for assumptions into your analysis. 





# ERGM

## Idea

- maximizing the entropy
- preserving the expected properties

### Grand canonical ensemble

Grand canonical means the overall features of all the microstates keep unchanged, which can be shown by the expected values of all the microstates.

For example, G(n,p) model gives us a lot of random networks. Taking the expectation of the edge number over these networks, we will get a fixed value np/2. The edge number is a grand canonical feature for this model.

## Properties

- fix the expected values of our interested properties:

  - ![img](http://latex.codecogs.com/svg.latex?%5Clangle%20f_i%5Crangle%20%3D%20%5Csum_G%20p%28G%29f_i%28G%29)

- the model is 

  - to maximize H(\Omega)
  - under the constraint  of probability sum 1 and the expected property value
  - take derivative for each microstate and set as 0

  ![img](http://latex.codecogs.com/svg.latex?%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20p%28G%29%7D%5C%7BH%2B%5Calpha%281-%5Csum_G%20p%28G%29%2B%5Csum_i%5Ctheta_i%28%5Clangle f_i%5Crangle-%5Csum_Gf_i%28G%29p%28G%29%29%5C%7D%3D0)

  we can get

  - ![img](http://latex.codecogs.com/svg.latex?%20p%28G%29%3D%5Cfrac%7B%5Cexp%28-%5Csum_i%5Ctheta_if_i%28G%29%29%7D%7BZ%7D)
  - ![img](http://latex.codecogs.com/svg.latex?Z%3D%5Csum_G%5Cexp%28-%5Csum_i%20%5Ctheta_if_i%28G%29%29)
  - ![img](http://latex.codecogs.com/svg.latex?%5Cfrac%7B%5Cpartial%20Z%7D%7B%5Cpartial%20%5Ctheta_i%7D%20%3D%20%5Csum_G-f_i%28G%29%5Cexp%28-%5Csum_j%5Ctheta_jf_j%28G%29%29%3D-%5Csum_G%20f_i%28G%29%20Z%5Cfrac%7B%5Cexp%28-%5Csum_j%5Ctheta_jf_j%28G%29%29%7D%7BZ%7D%20%3D%20-Z%5Clangle f_i%5Crangle)
  - ![img](http://latex.codecogs.com/svg.latex?%5Clangle f_i%5Crangle%3D-%5Cfrac%7B1%7D%7BZ%7D%5Cfrac%7B%5Cpartial%20Z%7D%7B%5Cpartial%20%5Ctheta_i%7D)

- we need to estimate theta

## Simulation algorithms

### Random walk

1. construct the transition matrix T: T_ij: the transition probability from microstate G_i to G_j
   1. walking sufficiently long such that visitation probabilities are close to stationary distribution
2. start with random network G_i
3. for random nodes v,w: 
   - if link (v,w) exists: remove
   - else, add
   - accept change with probability T_{v,w}
4. repeat large enough steps to generate a new network
5. calculate the expected properties on the newly generated networks, compared with the empirical network.

### MCMCMLE

1. choose initial value \theta_0
2. compute P(G|\theta_0) by generating enough microstates
3. randomly disturb \theta_0 to get \theta_1
4. repeat step 2 for \theta_1
5. compare the results of step 2 and 4. Keep the better parameter.
6. repeat step 2-5 until convergence.