---
title: Graph Convolutional Network
date: 2020-10-19
categories: [Learning Notes]
tags: [Graph, Deep Learning]
mathjax: true
---



- [x] what is the key idea of GCN?

- [x] what are learnable parameters in GCN?

- [x] what is the development path of GCN?

- [x] How to train a GCN?

- [x] How to write the GCN's code?

  

The key idea of GCN is to realize the convolution operator on graph by virtue of the spectral theory. 

# Development path

1. **Fourier Transformation** is defined in the graph field by Graph signal processing scholars.
   $$
   \hat{f}=U^\top f
   $$

2. **Graph convolution** is defined according to the **convolution theorem**.
   $$
   f\star h = \mathcal{F}^{-1}[\hat{f}(\omega)\hat{h}(\omega)] = U((U^\top h)\odot(U^\top f))
   $$

3. Graph convolution is applied in the **convolutional neural network**.

4. How to design the trainable convolutional kernel?

   1. [Spectral Networks and Locally Connected Networks on Graphs](https://arxiv.org/abs/1312.6203) Joan Bruna, Wojciech Zaremba, Arthur Szlam, Yann LeCun. ICLR2014
      $$
      y_{output}=\sigma(Ug_{\theta}(\Lambda)U^\top x)\\
      g_{\theta}(\Lambda) = diag(\theta_1,\cdots,\theta_n)
      $$
      There are n *learnable parameters* theta_i.

      

   2. Convolutional neural networks on graphs with fast localized spectral filtering.  Michae Ìˆl Defferrard, Xavier Bresson, and Pierre Vandergheynst. NIPS2016.
      $$
      g_{\theta}(\Lambda) = \sum_{j=0}^K\alpha_j\Lambda_j\\
      g_{\theta'}\star x\simeq \sum_{k=0}^K\theta'_kT_k(\tilde{L})x
      $$

      - There are k *learnable parameters* theta_i.
      - good spatial localization. K is the receptive field.

   3. [Semi-supervised classification with graph convolutional networks](https://arxiv.org/abs/1609.02907) Thomas N. Kipf, Max Welling. ICLR2017

      [Thomas Kipf code](https://github.com/tkipf/gcn)

      [dgl-code](https://github.com/dmlc/dgl/tree/master/examples/pytorch/gcn)

      Using Chebyshev Polynomial, let K=2 and theta_0 = -theta_1.

   $$
   g_{\theta}\star x\simeq \theta_0x-\theta_1D^{-1/2}AD^{-1/2}x=\theta(I_N+D^{-1/2}AD^{-1/2})x
   $$

      There is only one learnable parameter theta.

      Generalize x from 1-feature for each node to C-features for each node. For each feature, we use F filters. We have
   $$
      Z = \tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}X\Theta
   $$
      Thera are *CF learnable parameters* Theta.

     Chebyshev applied on diagonal matrix won't affect the matrix calculation.
   $$
   UT_k(\tilde{\Lambda})U = T_k(U\tilde{\Lambda}U)
   $$

     - the input of Chebyshev should be within [-1, 1]. So do the following transformation
       $$
       \tilde{\Lambda} = 2\Lambda/\lambda_{\max}-I
       $$

      - [x] why stacking multiple layers?

        - we can still recover a rich class of convolutional filter functions by stacking multiple such layers
        - Additionally, for a fixed computational budget, this layer-wise linear formulation allows us to build deeper models, a practice that is known to improve modeling capacity on a number of domains

$$
H^{(l+1)} = \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})
$$

# Refer

- https://zhuanlan.zhihu.com/p/199251885
- https://www.zhihu.com/question/54504471/answer/332657604



# Some other GNN resource

https://archwalker.github.io/blog/2019/11/10/GNN-Go-Through-Main-Models.html

https://zhuanlan.zhihu.com/p/133282394

:star: https://github.com/LiuChuang0059/Complex-Network

https://docs.dgl.ai/tutorials/models/index.html

Jure talk: https://raw.githubusercontent.com/LiuChuang0059/Complex-Network/master/GNN/tutorial/Advancements%20in%20Graph%20Neural%20Networks_graphSaGE_KG.pdf
