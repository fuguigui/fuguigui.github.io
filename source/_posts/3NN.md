---
title: Topic Model
date: 2019-07-09
categories: Computational Intelligence
---
# Topic Model
- Are the topics given ahead, fixed? Means we already have a set of available topics.
  - Yes. Also, in pLSA, all the words and documents are given.
- Are the given data regularized? Do they have the same dimension? Or have arbitrary length?
  - Yes, they have the same dimension. We need to pre-process data to achieve this.
- what are the approaches to solve this problem?
  - Key idea: Non-negative matrix factorization
  - practical method: EM algorithm
# Probabilistic Latent Semantic Analysis (pLSA)
- where shows **probabilistic**?
  - the probability of word_j in document_i is generated by topic_z.
- what is the **latent**?
	- the topics are latent variables.
- what does the **semantic** mean?
	- semantic means each topic has a fixed vocabulary distribution. This distribution is the semantic of this latent variable.
- what is the natural motivation of this model?
	- according to the "word appearance in this article" and all the "topics' word" distribution pattern, find the "topics of this article".
- how to do pLSA?
  - ideally, using MLE to solve the matrix V.
  - actually, using EM to solve both U and V.

## Model
Suppose the word appearance of a document is a probabilistic problem. This is wired, cause once a document is given, all the words are given, the p(word|docum) should be 1. In other words, a document is defined by all the words it includes.
We want to calculate this probability by the **conditional probability**, conditioned on the topics. That is how likely this document is under the specific topic? And how likely one word appears under this topic? Then we can get the probability of p(word|docum). p(topic|docum) is what we want to get.
### Objective
![img](http://latex.codecogs.com/svg.latex?p%28w%7Cd%29%3D%5CSigma_%7Bz%3D1%7D%5EKp%28w%7Cz%29p%28z%7Cd%29), z represents topics, w represents words, d represents document.

We want to maximize the p(w|d) by assigning p(z|d).

- How do we know p(w|z), given ahead?
  - ideally, yes. Actually no. We also need to solve this matrix.

### Data format
A matrix D: M\* N. M is the total number of different words, N is the total number of documents. D_ij is the occurrences of w_i in document d_j (or transposed)

- in the real-life case, will the M be very large? How should we decide N, how many documents we want to use?

In a matrix perspective to resay this problem, we 

1. decompose D as UV

2. U is given, we try to assign V to maximize the **log-likelihood** of D.

   ![img](http://latex.codecogs.com/svg.latex?%5Cmax%5CSigma_%7Bij%3D1%7Dx_%7Bij%7D%5Clog p%28w_i%7Cd_j%29)

-  U: M*T,  T is the number of topics. This is a given matrix, showing the word distribution of a topic. 
  - U_ij shows the word_i's appearance **probability** under the topic_j, should be non-negative ![img](http://latex.codecogs.com/svg.latex?U_%7Bij%7D%5Cgeq 0)
  - The sum of word distribution under a specific topic should be 1.![img](http://latex.codecogs.com/svg.latex?%5CSigma_%7Bi%3D1%7D%5EMU_%7Bij%7D%3D1)
- V: T*N. This is the matrix we want to assign. By assigning this matrix, we maximize the log-likelihood of D. V_ij shows the **ratio** of the topic_i in document_j. Document is a mixture of topics. We assume it composed by different topics.  Also, V should satisfy the 
  - non-negative
  - sum as 1.

## Challenge

The challenge is **U** is not given. We have **no prior** knowledge on it. To solve this problem, we 

- include one more variable ![img](http://latex.codecogs.com/svg.latex?q_%7Bzij%7D), denotes the probability that word j in document i is generated by topic z. ![img](http://latex.codecogs.com/svg.latex?%5CSigma_%7Bz%7Dq_%7Bzij%7D%3D1)
- use **EM** (expectation maximization) method.
  - expectation step, solve q. ![img](http://latex.codecogs.com/svg.latex?q_%7Bzij%7D%3D%5Cfrac%7Bp%28w_i%7Cz%29p%28z%7Cd_j%29%7D%7B%5CSigma_%7Bk%3D1%7D%5ETp%28w_i%7Ck%29p%28k%7Cd_j%29%7D)
  - maximization step, solve U and V. 
  	- ![img](http://latex.codecogs.com/svg.latex?u_%7Biz%7D%3D%5Cfrac%7B%5CSigma_j x_%7Bij%7Dq_%7Bzij%7D%7D%7B%5CSigma_%7Bi%7D%5CSigma_j x_%7Bij%7Dq_%7Bzij%7D%7D), the probability of topic z generates word i is the total "number" of word i generated by topic z across all the topics divided by the "number" of all the words generated by topic z across  and topics.
  	- ![img](http://latex.codecogs.com/svg.latex?v_%7Bzj%7D%3D%5Cfrac%7B%5CSigma_i x_%7Bij%7Dq_%7Bzij%7D%7D%7B%5CSigma_%7Bi%7D x_%7Bij%7D%7D), the ratio of topic z in document j is the number of words generated by topic z in document j divided by all the words in document j.

### remarks
- convergence guaranteed
- **not** for the final global optimum
- influenced by the initial value of U_0 and V_0, right?? Not unique??

# Latent Dirichlet Allocation (LDA)

- what is the different from pLSA
	- Here, we take a different attitude to Word\*topic matrix (essence) and Topic\*Docum(nusiance) matrix.  But in pLSA, these two matrices have the same level.
	- Also, in LDA it is convenient to judge for new documents.
- where shows Dirichlet?
	- In Docum\*Topic matrix, we assume topics follow a dirichlet distribution given a document.
	- In Word*Topic matrix, we assume words follow a dirichlet distribution given a topic.
- what does the allocation mean?
	- allocate topics to document by a dirichlet distribution

[zhihu](https://zhuanlan.zhihu.com/p/31470216)

## Dirichlet Distribution
- why we use dirichlet distribution here? Any advantage on computation? Or real-life closeness?
	- [Dirichlet distribution](https://zh.wikipedia.org/wiki/%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83) is used to describe the probability of each class i in totally K classes. It is for k classes continuous variables with the sum 1. It needs k parameters alphas.
	- Mwe use this cause it is good for the distribution of "probability".
	- it is the "simplest" **conjugate** distribution. This is very nice!!! It means the prior and posterior are from the same distribution family.
	  - Gaussian Distribution is conjugate prior/posterior with respect to Gaussian likelihood.
	  - with the multinomial likelihood, Dirichlet is the choice!

## Model
- how to solve the matrices U and V?
  - in LDA, there are two steps: 
    - achieve V, many methods:
    	- MCMC
    	- Variational EM
    	- ...
    - for each new document **vector x** = (x1,x2, ... xM), the word count across all the possible word, we try to find the topic **vector u**=(u1, u2, ..., uK).

Since **vector u** is not fixed, is some way random, we assume it follows a **Dirichlet** distribution, and the posterior **vector x** also follows a **Dirichlet** distribution. Then we apply Bayesian approach to solve **u** according to **v**.

# Non-Negative Matrix Factorization(NMF)
- what is NMF?
	- all the entries of the decomposed matrix are non-negative and L1 normalization.
- what is the application situation of NMF?
	- to solve the probabilistic matrix by reducing dimension.
## Objective
- log-likelihood objective when modeling for probability. That is pLSA
- quadratic cost

